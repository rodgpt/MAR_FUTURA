{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fcc207b",
   "metadata": {},
   "source": [
    "# RunModel\n",
    "\n",
    "This notebook embeds all WAV files in a folder into a Hoplite DB, loads a previously-trained AGILE linear classifier, and writes an inference CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e26a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "from etils import epath\n",
    "import os\n",
    "\n",
    "from perch_hoplite.agile import audio_loader\n",
    "from perch_hoplite.agile import classifier\n",
    "from perch_hoplite.agile import colab_utils\n",
    "from perch_hoplite.agile import embed\n",
    "from perch_hoplite.agile import source_info\n",
    "from perch_hoplite.agile.classifier import LinearClassifier\n",
    "from perch_hoplite.zoo import model_configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f23efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Configuration { vertical-output: true }\n",
    "\n",
    "# -----------------------------\n",
    "# PATHS (Local vs Colab)\n",
    "# -----------------------------\n",
    "\n",
    "# For running locally (Rod)\n",
    "base_agile_path = epath.Path(\n",
    "    '/Users/Rodrigo/Library/CloudStorage/GoogleDrive-royanedel@marfutura.org/Mi unidad/Agile'\n",
    ")\n",
    "\n",
    "# For running in Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# base_agile_path = epath.Path('/content/drive/Shareddrives/MAR FUTURA/Agile')\n",
    "\n",
    "# -----------------------------\n",
    "# USER SETTINGS\n",
    "# -----------------------------\n",
    "\n",
    "# Folder containing audio to classify.\n",
    "input_audio_dir = \"/Users/Rodrigo/Library/CloudStorage/GoogleDrive-royanedel@marfutura.org/Unidades compartidas/MAR FUTURA/Hydrophones/Matanzas/13-11-25/32\"\n",
    "dataset_name = 'RunDataset'  # @param {type:'string'}\n",
    "dataset_fileglob = '*.[wW][aA][vV]'  # @param {type:'string'}\n",
    "\n",
    "# Where to store the embedding DB for this run.\n",
    "db_path = \"/Users/Rodrigo/Library/CloudStorage/GoogleDrive-royanedel@marfutura.org/Unidades compartidas/MAR FUTURA/Hydrophones/Matanzas/13-11-25/32\"\n",
    "\n",
    "# Saved classifier created in CreateModel.ipynb (LinearClassifier.save).\n",
    "classifier_path = str(base_agile_path / 'Data' / 'agile_classifier_v2.pt')  # @param {type:'string'}\n",
    "\n",
    "# Output CSV path.\n",
    "output_csv_filepath = str(base_agile_path / 'RunResults' / 'inference.csv')  # @param {type:'string'}\n",
    "\n",
    "# Embedding model choice MUST match how you embedded when you trained the classifier.\n",
    "model_choice = 'perch_8'  #@param['perch_v2','perch_8', 'humpback', 'multispecies_whale', 'surfperch', 'birdnet_V2.3']\n",
    "\n",
    "# Optional sharding (keep consistent with training if possible).\n",
    "use_file_sharding = True  # @param {type:'boolean'}\n",
    "shard_length_in_seconds = 5  # @param {type:'number'}\n",
    "\n",
    "# Performance knobs\n",
    "# - audio_worker_threads: parallel audio loading/processing\n",
    "# - embed_batch_size: how many sources are queued per dispatch\n",
    "# If you overload your machine, lower these.\n",
    "audio_worker_threads = 8  # @param {type:'integer'}\n",
    "embed_batch_size = 32  # @param {type:'integer'}\n",
    "\n",
    "# Inference threshold. Higher => fewer detections.\n",
    "logit_threshold = 2  # @param\n",
    "labels = None  # @param\n",
    "\n",
    "# Create output folder.\n",
    "epath.Path(output_csv_filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "epath.Path(db_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "audio_glob = source_info.AudioSourceConfig(\n",
    "    dataset_name=dataset_name,\n",
    "    base_path=input_audio_dir,\n",
    "    file_glob=dataset_fileglob,\n",
    "    min_audio_len_s=1.0,\n",
    "    target_sample_rate_hz=-2,\n",
    "    shard_len_s=float(shard_length_in_seconds) if use_file_sharding else None,\n",
    ")\n",
    "\n",
    "configs = colab_utils.load_configs(\n",
    "    source_info.AudioSources((audio_glob,)),\n",
    "    db_path,\n",
    "    model_config_key=model_choice,\n",
    "    db_key='sqlite_usearch',\n",
    ")\n",
    "\n",
    "# Correcting the model handle for surfperch\n",
    "if model_choice == 'surfperch':\n",
    "  configs.model_config.model_config.tfhub_path = 'google/surfperch/1'\n",
    "\n",
    "print('input_audio_dir:', input_audio_dir)\n",
    "print('db_path:', db_path)\n",
    "print('classifier_path:', classifier_path)\n",
    "print('output_csv_filepath:', output_csv_filepath)\n",
    "print('audio_worker_threads:', audio_worker_threads)\n",
    "print('embed_batch_size:', embed_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Embed folder, load classifier, and run inference { vertical-output: true }\n",
    "\n",
    "# 1) Connect/create DB\n",
    "db = configs.db_config.load_db()\n",
    "print('Initialized DB located at', configs.db_config.db_config.db_path)\n",
    "\n",
    "# 2) Embed all files in the folder\n",
    "print(f'Embedding dataset: {audio_glob.dataset_name}')\n",
    "worker = embed.EmbedWorker(\n",
    "    audio_sources=configs.audio_sources_config,\n",
    "    db=db,\n",
    "    model_config=configs.model_config,\n",
    "    audio_worker_threads=int(audio_worker_threads),\n",
    ")\n",
    "worker.process_all(target_dataset_name=audio_glob.dataset_name, batch_size=int(embed_batch_size))\n",
    "print('Embedding complete, total embeddings:', db.count_embeddings())\n",
    "\n",
    "# 3) Load embedding model (needed for audio loader in some workflows; kept for parity)\n",
    "db_model_config = db.get_metadata('model_config')\n",
    "embed_config = db.get_metadata('audio_sources')\n",
    "model_class = model_configs.get_model_class(db_model_config.model_key)\n",
    "embedding_model = model_class.from_config(db_model_config.model_config)\n",
    "audio_sources = source_info.AudioSources.from_config_dict(embed_config)\n",
    "window_size_s = getattr(embedding_model, 'window_size_s', 5.0)\n",
    "_ = audio_loader.make_filepath_loader(\n",
    "    audio_sources=audio_sources,\n",
    "    window_size_s=window_size_s,\n",
    "    sample_rate_hz=embedding_model.sample_rate,\n",
    ")\n",
    "\n",
    "# 4) Load trained classifier and write inference CSV\n",
    "linear_classifier = LinearClassifier.load(classifier_path)\n",
    "classifier.write_inference_csv(\n",
    "    linear_classifier,\n",
    "    db,\n",
    "    output_csv_filepath,\n",
    "    logit_threshold,\n",
    "    labels=labels,\n",
    ")\n",
    "print('Done. Wrote:', output_csv_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot detections over time (detections/hour)\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "csv_path = output_csv_filepath\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print('rows:', len(df))\n",
    "print('columns:', list(df.columns))\n",
    "\n",
    "# Optional: focus on one label (e.g. boat). Set to None to include all labels.\n",
    "focus_label = 'boat'  # @param {type:'string'}\n",
    "if focus_label and 'label' in df.columns:\n",
    "  df = df[df['label'] == focus_label]\n",
    "\n",
    "# Parse datetime from filename like YYYYMMDD_HHMMSS(.WAV)\n",
    "# Example: ZAPALLAR_20241122_143550_5sec.wav -> 20241122_143550\n",
    "_dt_re = re.compile(r'(\\d{8})_(\\d{6})')\n",
    "\n",
    "def extract_dt(fname: str):\n",
    "  m = _dt_re.search(str(fname))\n",
    "  if not m:\n",
    "    return pd.NaT\n",
    "  return pd.to_datetime(m.group(1) + m.group(2), format='%Y%m%d%H%M%S', errors='coerce')\n",
    "\n",
    "df['file_dt'] = df['filename'].apply(extract_dt)\n",
    "\n",
    "# If window_start exists, shift timestamp by that many seconds.\n",
    "if 'window_start' in df.columns:\n",
    "  df['window_start'] = pd.to_numeric(df['window_start'], errors='coerce')\n",
    "  df['dt'] = df['file_dt'] + pd.to_timedelta(df['window_start'].fillna(0), unit='s')\n",
    "else:\n",
    "  df['dt'] = df['file_dt']\n",
    "\n",
    "# Drop rows where we can't parse time\n",
    "plot_df = df.dropna(subset=['dt']).copy()\n",
    "if plot_df.empty:\n",
    "  raise RuntimeError('No rows had a parseable datetime in filename. Adjust extract_dt() regex/format.')\n",
    "\n",
    "plot_df = plot_df.set_index('dt').sort_index()\n",
    "\n",
    "detections_per_hour = plot_df['idx'].resample('1H').count()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "detections_per_hour.plot()\n",
    "plt.title(f'Detections per hour' + (f\" ({focus_label})\" if focus_label else ''))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Detections / hour')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: show daily totals too\n",
    "show_daily = False  # @param {type:'boolean'}\n",
    "if show_daily:\n",
    "  daily = plot_df['idx'].resample('1D').count()\n",
    "  plt.figure(figsize=(12, 4))\n",
    "  daily.plot()\n",
    "  plt.title(f'Detections per day' + (f\" ({focus_label})\" if focus_label else ''))\n",
    "  plt.xlabel('Date')\n",
    "  plt.ylabel('Detections / day')\n",
    "  plt.grid(True, alpha=0.3)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
